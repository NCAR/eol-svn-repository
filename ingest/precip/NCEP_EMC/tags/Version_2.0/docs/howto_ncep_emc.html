<html>
<head>
<title>NCEP/EMC Processing</title>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<link rel="stylesheet" type="text/css" href="../dpg_howto.css">
</head>

<body>
<div class="howto_title">How To Download and Archive NCEP/EMC Precipitation Data</div>
<hr>
<table width=100%>
<tr><td class="nowrap">
<h1>Table of Contents</h1><ol>
	<li><a href="#setup">Setup to Processing and Background</a><ol>
		<li><a href="#setup_about">About the Data</a></li>
		<li><a href="#setup_types">Types of Data</a></li>
		<li><a href="#setup_datasets">The NCEP Datasets</a></li>
		<li><a href="#setup_locations">Data Locations</a></li>
		</ol></li>
	<li><a href="#scripts">The Scripts</a><ol>
		<li><a href="#scripts_main">Main Scripts</a></li>
		    <li><a href="#scripts_aux">Auxiliary Scripts</a></li>
		</ol></li>
	<li><a href="#modules">The Modules</a><ol>
		<li><a href="#modules_ftp">NCEP FTP Module</a></li>
		<li><a href="#modules_archive">NCEP Archive Module</a></li>
		<li><a href="#modules_database">NCEP Database Modules</a></li>
		<li><a href="#modules_util">NCEP Utility Module</a></li>
		<li><a href="#modules_mysql">MySql Database Modules</a></li>
		</ol></li>
	<li><a href="#process">The Process</a><ol>
		<li><a href="#process_download">Downloading the Data</a></li>
		<li><a href="#process_archive">Archiving the Data</a></li>
		<li><a href="#process_uncompress">Uncompressing the Data</a></li>
		<li><a href="#process_database">Putting the Data in the Database</a></li>
		</ol></li>
	<li><a href="#notes">Other Notes</a></li>
</ol>
</td><td class="nopanic">
<h1 class="nopanic_title">Don't Panic</h1>
<p>If you want to jump right in and find how to run the script for the month, go <a href="#scripts_main">here</a>.</p>
<p>If you are looking for the Sid Katz data, you have come to right place.  It has been renamed so it reflects the data and not the person who it comes from.</p>
</td></tr>
</table>
<hr>

<a name="setup"></a><h2>Setup and Processing</h2>
<a name="setup_about"></a><h3>About the Data</h3>
<p>The <a href="http://www.ncep.noaa.gov">National Center for Environmental Prediction 
  (NCEP)</a> <a href="http://www.emc.ncep.noaa.gov">Environment Modeling Center 
  (ECM)</a> provides precipitation data that JOSS downloads, archives, and places 
  into the database. The process is run every month, around the second week of 
  the month, once Janine receives an e-mail saying the data is ready. The data 
  can be found on the NCEP server <a href="ftp://ftpprd.ncep.noaa.gov/pub/gcp/precip/JOSS">ftpprd.ncep.noaa.gov/pub/gcp/precip/JOSS</a>.</p>
<p class="important">This is the Sid Katz data.</p>

<a name="setup_types"></a><h3>The Types of Data</h3>
<p>There are six types of files that JOSS downloads.  They are:<ul>
	<li><b>Ceopavn Files</b>-  There are two files that follow the pattern <code>ceopavn.YYYYMMDDHH</code>.</li>
	<li><b>Daily Precipitation Files</b>- There is a single file that follows the pattern <code>prcp.dly.mmmYYYY</code>.  The file contains the gage files for each day of the month.</li>
	<li><b>Hourly Precipitation Files</b>- There is a single file that follows the pattern <code>prcp.hrly.mmmYYYY</code>.  The file contains the gage files for each day of the month.</li>
	
  <li><b>Snapshot Imagery Files</b>- There is a single file that follows the pattern 
    <code>snapshot.mmmYYYY</code>. The file contains GIF imagery.</li>
	<li><b>ST2 4 KM Files</b>- There is a single file for each day of the month that follows the pattern <code>ST2_4km.YYYYMMDD</code>.  The files contain GRIB data files.</li>
	<li><b>Stage IV Files</b>- There is a single file that follows the pattern <code>stage4.mmmYYYY</code>.  The file contains data files and GIF imagery.</li>
	</ul>
<p>The following list explains the character replacement in the above file patterns.<ul>
	<li><b>DD</b>- The day of the month for the data in the file.</li>
	<li><b>HH</b>- The hour of the day for the data in the file.  The hour is either 00 or 12.</li>
	<li><b>MM</b>- The month of the data in the file.</li>
	<li><b>YYYY</b>- The year of the data in the file.</li>
	<li><b>mmm</b>- The first three letters of the month (all lowercase) for the data in the file.</li>
	</ul>

<a name="setup_datasets"></a><h3>The NCEP Datasets</h3>
<p>The following is the list of datasets where the NCEP/ECM data is loaded.  The first part is the dataset id number and the name of the dataset.  The second part is which files are inserted into that dataset and which of the downloaded files they came from.</p>
<ul>
	<li><dt><a href="/cgi-bin/codiac/dss?21.004">21.004</a> Precipitation NCEP/EMC Gage Only Hourly Dataset</dt><dd>One file for each day of the month (gage.hrly.prcp.*).  The files come from the hourly precipitation file.</dd></li>
	<li><dt><a href="/cgi-bin/codiac/dss?21.005">21.005</a> Precipitation NCEP/EMC Gage Only Daily Dataset</dt>
    <dd>One file for each day of the month (gage.dly.prcp.*). The files come from 
      the daily perception file.</dd>
  </li>
	<li><dt><a href="/cgi-bin/codiac/dss?21.087">21.087</a> Precipitation NCEP/EMC Preview Imagery</dt><dd>Several GIF files for each day with hourly, six hourly, and daily time periods.  The files come from the snapshot imagery file and the stage IV file.</dd></li>
	<li><dt><a href="/cgi-bin/codiac/dss?21.088">21.088</a> Precipitation NCEP/EMC 4KM Gridded Data (GRIB) Gage Only Analysis (GG)</dt><dd>Several compressed (.Z) GRIB files for each day with hourly, six hourly, and daily time periods.  The files come from the ST2 4 KM day files.</dd></li>
	<li><dt><a href="/cgi-bin/codiac/dss?21.089">21.089</a> Precipitation NCEP/EMC 4KM Gridded Data (GRIB) Multi-Sensor Analysis (ML)</dt>
    <dd>Several compressed (.Z) GRIB files for each day with hourly, six hourly, 
      and daily time periods. The files come from the ST2 4 KM day files.</dd>
  </li>
	<li><dt><a href="/cgi-bin/codiac/dss?21.090">21.090</a> Precipitation NCEP/EMC 4KM Gridded Data (GRIB) Radar Est. no bias removed (RD)</dt>
    <dd>Several compressed (.Z) GRIB files for each day with hourly, six hourly, 
      and daily time periods. The files come from the ST2 4 KM day files.</dd>
  </li>
	<li>
    <dt><a href="/cgi-bin/codiac/dss?21.091">21.091</a> Precipitation NCEP/EMC 
      4KM Gridded Data (GRIB) Gage-Only 24h accumulated RFC</dt>
    <dd>One compressed (.Z) GRIB file for each day of the month.  The files come from the ST2 4 KM day files.</dd></li>
	<li><dt><a href="/cgi-bin/codiac/dss?21.092">21.092</a> Precipitation NCEP/EMC 4KM Gridded Data (GRIB) Radar Est. w/bias removal UN</dt><dd>Several compressed (.Z) GRIB files for each day with hourly, six hourly, and daily time periods.  The files come from the ST2 4 KM day files.</dd></li>
	<li><dt><a href="/cgi-bin/codiac/dss?21.093">21.093</a> Precipitation NCEP/EMC 4KM Gridded Data (GRIB) Stage IV Data</dt><dd>Several compressed (.Z) GRIB files for each day with hourly, six hourly, and daily time periods.  The files come from the Stage IV file.</dd></li>
</ul>

<a name="setup_locations"></a><h3>Data Locations</h3>
<table border=2>
<tr><th>/archive/codiac/eop/katz_data</th><td>The directory where the data is stored for the database.</td></tr>
<tr><th>/web/data/eop/katz_data</th><td>The directory where the imagery is stored for the database so it can be viewed online.</td></tr>
<tr><th>/work/operational/surface/NCEP_EMC/archive</th><td>The directory where tar balls are stored before they are moved to the mass store.</td></tr>
<tr><th>/work/operational/surface/NCEP_EMC/docs</th><td>The directory where documentation is stored for the NCEP/EMC data.</td></tr>
<tr><th>/work/operational/surface/NCEP_EMC/ingest</th><td>The directory where the data is downloaded and expanded.  It is divided into year/month directories.</td></tr>
<tr><th>/work/operational/surface/NCEP_EMC/lib</th><td>The directory where the modules are stored.</td></tr>
<tr><th>/work/operational/surface/NCEP_EMC/logs</th><td>The directory where the logs from the scripts/modules are placed.  It is divided into year directories.</td></tr>
<tr><th>/work/operational/surface/NCEP_EMC/software</th><td>The directory where the scripts are stored.</td></tr>
<tr><th>mss:/DSS/DS507.5/updates</th><td>The mass store directory where the katz tar ball is stored for Chi-Fan.</td></tr>
<tr><th>mss:/JOSS/DATA/RAW/BY_PROJECT/CEOP</th><td>The mass store directory where the ceopavn tar balls are stored.  It is divided into year directories.</td></tr>
</table>
<hr>

<a name="scripts"></a><h2>The Scripts</h2>
<a name="scripts_main"></a><h3>The Main Scripts</h3>
<p>The main scripts are the set of scripts that perform the automatic processing of the NCEP data.  They are meant to be run once a month and perform all of the steps need to process the data.  It is not meant to be used to do partial processing.</p>
<h4>process_ncep.pl</h4>
<p>The <b>process_ncep.pl</b> script is the main script.  It controls the entire processing from downloading the data, to archiving it to the mass store, and putting it into the database.  It should be run on <b>tornado</b>, but it does not have to run on it.  For more information on how the script works, see <a href="#process">the process</a> section of this document.  It should be used as follows:</p>
<div class="code">process_ncep.pl MM YYYY your_email@address</div>
<p>The first command will process the NCEP data for the previous from the current day the script is run.  The second command will process the NCEP data for the specified month and year where <b>MM</b> is the month and <b>YYYY</b> is the year.  <font class="important">The script needs to be run as a user has access to run the SQL commands (member of the <i>storm</i> group).  It is expected to run as the <i>joss</i> user.</font></p>
<p><font class="important"><b>If you are just doing monthly loading, and the script completes without any errors, you are done.</b></font></p>
<p>This script is setup to attempt to do all of the commands in a specific step, 
  i.e. download all of the data or insert all of the files, even if an error occurs. 
  This is to allow each step to perform the maximum amount of steps before having 
  to stop. At the end of each step, if an error occurred, the script will send 
  out an e-mail with the error report and will quit. This prevents the script 
  from trying to archive the data if it has not all been downloaded or insert 
  the data into the database if the files have not been uncompressed. If an error 
  occurs, the auxiliary scripts will need to be run to finish the processing. 
  If everything went as it was suppose to, the script will send out a completion 
  message to tell the users that the processing has completed.</p>

<h4>ncep_empsql.pl</h4>
<p>The <b>ncep_empsql.pl</b> script is for doing the EmpSQL database entry for the <b>process_ncep.pl</b> script.  This is needed because the database entry needs to be done on <b>hurricane</b> and the rest of the process on <b>tornado</b>.  This is done by a <i>system</i> call to this script from <b>process_ncep.pl</b> to run it on <b>hurricane</b>.  This should not be run on its own.</p>

<h4>ncep_mysql.pl</h4>
<p>The <b>ncep_mysql.pl</b> script is for doing the MySQL database entry for the <b>process_ncep.pl</b> script.  This is needed because the database entry needs to be done on <b>hurricane</b> and the rest of the process on <b>tornado</b>.  This is done by a <i>system</i> call to this script from <b>process_ncep.pl</b> to run it on <b>hurricane</b>.  This should not be run on its own.</p>

<a name="scripts_aux"></a>
<h3>Auxiliary Scripts</h3>
<p>The auxiliary scripts are to be used when an error has occurred in the main 
  running of the program. This allows the user to pick up the process at the point 
  of the error, correcting the problem, and finishing the processing.</p>
<p class="important">If any of the auxiliary scripts are run, they will overwrite 
  any of the log files that have previously been created. The user should go to 
  the <b>log</b> directory and rename the files so they will can be kept for a 
  record of what has happened.</p>
<h4>ncep_aux_ftp.pl</h4>
<p>The <b>ncep_aux_ftp.pl</b> script allows the user to have more control over the FTP download of the NCEP data.  It can be used as follows:</p>
<div class="code">ncep_aux_ftp.pl MM YYYY</div>
<div class="code">ncep_aux_ftp.pl MM YYYY all</div>
<div class="code">ncep_aux_ftp.pl MM YYYY hrly dly snapshot stage4 st2_4km ceopavn</div>
<p>The <b>MM</b> is the month to be downloaded and <b>YYYY</b> is the year.  The first two commands will download all of the data for the specified month.  The last command will download all of the data for the types specified.  The code <b>hrly</b> will download the hourly file, <b>dly</b> will download the daily file, <b>snapshot</b> will download the snapshot file, <b>stage4</b> will download the stage4 file, <b>st2_4km</b> will download the st2_4km files, and <b>ceopavn</b> will download the ceopavn files.  The codes can be given in any order or combination.</p> 

<h4>ncep_aux_archive.pl</h4>
<p>The <b>ncep_aux_archive.pl</b> script allows the user to have more control over the archiving of the NCEP data into tar balls and placing them on the mass store.  It can be used as follows:</p>
<div class="code">ncep_aux_archive.pl MM YYYY</div>
<div class="code">ncep_aux_archive.pl MM YYYY all</div>
<div class="code">ncep_aux_archive.pl MM YYYY ceopavn ncep</div>
<p>The <b>MM</b> is the month to be archived and <b>YYYY</b> is the year. The 
  first two commands will archive all of the data for the specified month. The 
  last command will archive the specified type of data into a tar ball and place 
  it on the mass store. The code <b>ceopavn</b> will archive the ceopavn files 
  and the code <b>ncep</b> will archive all of the other data (hourly, daily, 
  snapshot, stage4, and st2_4km). If the <b>ncep</b> code is given, it will send 
  an e-mail to Chi-Fan saying the file is on the mass store.</p>

<h4>ncep_aux_uncompress.pl</h4>
<p>The <b>ncep_aux_uncompress.pl</b> script allows the user to have more control over the uncompressing of the downloaded data as it is being prepared for the database.  It can be used as follows:</p>
<div class="code">ncep_aux_uncompress.pl MM YYYY</div>
<div class="code">ncep_aux_uncompress.pl MM YYYY all</div>
<div class="code">ncep_aux_uncompress.pl MM YYYY dly hrly snapshot st2_4km stage4</div>
<p>The <b>MM</b> is the month to be uncompressed and <b>YYYY</b> is the year. 
  The first two commands will uncompress all of the data for the specified month. 
  The last command will uncompress the specified data into its own directory under 
  the monthly ingest directory. The code <b>dly</b> will uncompress the daily 
  precipitation gage files, <b>hrly</b> will uncompress the hourly precipitation 
  gage files, <b>snapshot</b> will uncompress the snapshot imagery tar ball, <b>st2_4km</b> 
  will uncompress the st2_4km day file tar balls, and <b>stage4</b> will uncompress 
  the stage4 tar ball.</p>

<h4>ncep_aux_mysql.pl</h4>
<p>The <b>ncep_aux_mysql.pl</b> script allows the user to have more control over the inserting of the NCEP data into the MySQL database.  It can be used as follows:</p>
<div class="code">ncep_aux_mysql.pl MM YYYY</div>
<div class="code">ncep_aux_mysql.pl MM YYYY all</div>
<div class="code">ncep_aux_mysql.pl MM YYYY dly hrly snapshot st2_4km stage4</div>
<p>The <b>MM</b> is the month to be inserted and <b>YYYY</b> is the year.  The first two commands will insert all of the data for the specified month.  The last command will insert the specified data types into their appropriate datasets.  The code <b>dly</b> will insert the daily gage files, <b>hrly</b> will insert the hourly gage files, <b>snapshot</b> will insert the snapshot GIF imagery, <b>st2_4km</b> will insert the 4 KM GRIB data, and <b>stage4</b> will insert the stage IV GIF imagery and GRIB data files.</p>

<h4>ncep_aux_empsql.pl</h4>
<p>The <b>ncep_aux_empsql.pl</b> script allows the user to have more control over the inserting of the NCEP data into the EmpSQL database.  It can be used as follows:</p>
<div class="code">ncep_aux_empsql.pl MM YYYY</div>
<div class="code">ncep_aux_empsql.pl MM YYYY all</div>
<div class="code">ncep_aux_empsql.pl MM YYYY dly hrly snapshot st2_4km stage4</div>
<p>The <b>MM</b> is the month to be inserted and <b>YYYY</b> is the year.  The first two commands will insert all of the data for the specified month.  The last command will insert the specified data types into their appropriate datasets.  The code <b>dly</b> will insert the daily gage files, <b>hrly</b> will insert the hourly gage files, <b>snapshot</b> will insert the snapshot GIF imagery, <b>st2_4km</b> will insert the 4 KM GRIB data, and <b>stage4</b> will insert the stage IV GIF imagery and GRIB data files.</p>

<hr>

<a name="modules"></a><h2>The Modules</h2>
<p>There are several modules that were created to help in the NCEP processing.  They are listed below with descriptions to their purpose.  All of the modules can be found in the <b>lib</b> directory.</p>

<a name="modules_ftp"></a><h3>NCEP FTP Module</h3>
<p>The FTP module is used for connecting to the NCEP FTP server and downloading the data.  It contains the controls for connecting to server and downloading the data.  Most of the functions will return a String with either an error message if it was not executed properly or the empty String if nothing unexpected occurred.  The documentation on the individual module can be found <a href="/cgi-bin/dpg/doc/docperl.cgi?file=/work/./NCEP_EMC/lib/NcepFTP.pm">here</a>.  A more verbose explanation of the FTP process can be found in the <a href="#process_download">downloading process</a> section of this document.</p>

<a name="modules_archive"></a><h3>NCEP Archive Module</h3>
<p>The Archive module is used for creating the tar balls of the NCEP data, placing 
  them on the mass store, e-mailing Chi-Fan, and uncompressing the data so it 
  can be put into the database. Most of the functions will return a String with 
  either an error message if it was not executed properly or the empty String 
  if nothing unexpected occurred. The documentation on the individual module can 
  be found <a href="/cgi-bin/dpg/doc/docperl.cgi?file=/work/./NCEP_EMC/lib/NcepArchive.pm">here</a>. 
  A more verbose explanation of the archive process can be found in the <a href="#process_archive">archiving 
  process</a> section and the <a href="#process_uncompress">uncompressing process</a> 
  section of this document.</p>

<a name="modules_database"></a><h3>NCEP Database Modules</h3>
<p>There are two different database modules, one for EmpSQL (<b>NcepEmpsql.pm</b>) 
  and one for MySQL (<b>NcepMysql.pm</b>). They have the same functions and the 
  same functionality, but work on the different types of databases. They handle 
  the moving of the data into the archive directory for the database and inserting 
  the files into the tables. Most of the functions will return a String with either 
  an error message if it was not executed properly or the empty String if nothing 
  unexpected occurred. The documentation on the individual modules can be found 
  <a href="/cgi-bin/dpg/doc/docperl.cgi?file=/work/./NCEP_EMC/lib/NcepEmpsql.pm">here</a> 
  for the EmpSQL version and <a href="/cgi-bin/dpg/doc/docperl.cgi?file=/work/./NCEP_EMC/lib/NcepMysql.pm">here</a> 
  for the MySQL version. A more verbose explanation of the database process can 
  be found in the <a href="#process_database">database process</a> section of 
  this document.</p>

<a name="modules_util"></a><h3>NCEP Utility Module</h3>
<p>The utility module (<b>NcepUtil</b>) is a module that contains general NCEP 
  processing information and functions. It contains constants such as directory 
  names and e-mail addresses. It also contains shared functions like date manipulation 
  and emailing. The documentation on the module can be found <a href="/cgi-bin/dpg/doc/docperl.cgi?file=/work/./NCEP_EMC/lib/NcepUtil.pm">here</a>.</p>

<a name="modules_mysql"></a><h3>MySQL Database Modules</h3>
<p>There are three modules that are used for the connection to the MySQL database. 
  They provide the connectivity to the database along with objects the represent 
  a dataset and a file for their associated tables. The documentation for these 
  modules are in a separate document on how they can be used. The document can 
  be found <a href="/dpg/???/howto_mysql_modules.html">here</a>.</p>

<hr>

<a name="process"></a><h2>The Process</h2>
<p>The following is a detailed version of the steps to process the NCEP precipitation data.  It explains the steps taken by the scripts and modules and how it can be done manually if that was needed.</p>

<a name="process_download"></a><h3>Downloading the Data</h3>
<p>The first step to the processing is downloading the data from the NCEP server. 
  The server name, login information, and directory where the data can be retrieved 
  are:</p>
<div class="code">ftpprd.ncep.noaa.gov</div>
<div class="code">anonymous (e-mail address)</div>
<div class="code">/pub/gcp/precip/JOSS</div>

<ol>
	<li><p class="command">Log on to the server and change to the data directory.</p>
		<p>Using the above information, the user will log on to the server and change to the data directory.</p>
		<p>In the <b>NcepFTP.pm</b> module, this step is done by calling the <b>open</b> function.  (A new object will need to be created using the month and year before the function can be called.)</p>
		</li>
	<li><p class="command">Create Month Directory</p>
		<p>The directory where the files should be downloaded should be created in the pattern of mmmYYYY where the <b>mmm</b> is the first three letters of the month (all lowercase) and <b>YYYY</b> is the year.  All of the files should be stored in this directory except the ceopavn.  (The ceopavn can be put into this directory, but need to be removed before the katz tar ball is created.)</p>
	<li><p class="command">Download the Hourly Precipitation file.</p>
		<p>Get the file: <code>prcp.hrly.mmmYYYY</code> where <b>mmm</b> is the first three letters of the month (all lowercase) and <b>YYYY</b> is the year.  The file will be placed in the current working directory of the user.</p>
		<p>The module will download this file using the <b>download_hourly_precip()</b> function.  The file will be placed in the <b>ingest</b> directory for the month and year.</p>
		</li>
	<li><p class="command">Download the Daily Precipitation file.</p>
		<p>Get the file: <code>prcp.dly.mmmYYYY</code> where <b>mmm</b> is the first three letters of the month (all lowercase) and <b>YYYY</b> is the year.  The file will be placed in the current working directory of the user.</p>
		<p>The module will download this file using the <b>download_daily_precip()</b> function.  The file will be placed in the <b>ingest</b> directory for the month and year.</p>
		</li>
	<li><p class="command">Download the Snapshot file.</p>
		<p>Get the file: <code>snapshot.mmmYYYY</code> where <b>mmm</b> is the first three letters of the month (all lowercase) and <b>YYYY</b> is the year.  The file will be placed in the current working directory of the user.</p>
		<p>The module will download this file using the <b>download_snapshot()</b> function.  The file will be placed in the <b>ingest</b> directory for the month and year.</p>
		</li>
	<li><p class="command">Download the Stage IV file.</p>
		<p>Get the file: <code>stage4.mmmYYYY</code> where <b>mmm</b> is the first three letters of the month (all lowercase) and <b>YYYY</b> is the year.  The file will be placed in the current working directory of the user.</p>
		<p>The module will download this file using the <b>download_stage4()</b> function.  The file will be placed in the <b>ingest</b> directory for the month and year.</p>
		</li>
	<li><p class="command">Download the Stage II 4 KM files.</p>
		<p>Get the file: <code>ST2_4km.YYYYMM*</code> where <b>YYYY</b> is the year and <b>MM</b> is the month.  There should be one file for each day of the month.  The files will be placed in the current working directory of the user.</p>
		<p>The module will download these files using the <b>download_st2_4km()</b> function.  The files will be placed in the <b>ingest</b> directory for the month and year.</p>
		</li>
	<li><p class="command">Download the Ceopavn files.</p>
		<p>Get the file: <code>ceopavn.YYYYMM*</code> where <b>YYYY</b> is the year and <b>MM</b> is the month.  There should be two files for each day of the month (00 and 12 hours).  The files will be placed in the current working directory of the user.</p>
		<p>The module will download these files using the <b>download_ceopavn()</b> function.  The files will be placed in the <b>ingest</b> directory for the month and year.</p>
		</li>
	<li><p class="command">Close the Connection</p>
		<p>Terminate the connection to the server.</p>
		
    <p>This is done with the <b>close</b> function in the module. <font class="important">This 
      needs to be done so the connection will close cleanly. It will eventually 
      time-out, but will hold onto resources on both the server and the local 
      machine until it times out. It also needs to be called to close the log 
      file generated by the module.</font></p>
		</li>
</ol>
<p>Here are some other general notes about the downloading of the data.</p>
<ul>
	<li>The module will generate a log file which will be put into the <b>log</b> directory for the year.  The file is named: <code>FTPYYYYMM.log</code> where the <b>YYYY</b> is the year and <b>MM</b> is the month.  It contains all of the commands that were executed during the processing.</li>
	<li>The downloading process is slow.  The files are pretty large ranging from around 5 MB for some of the day files to 60 MB for some of the month files.  Do not be surprised if this takes over an hour while using the module.</li>
</ul>

<a name="process_archive"></a><h3>Archiving the Data</h3>
<p>Archiving the data is the process of creating tar balls of the downloaded files and placing them on the mass store.  Two files are created.  One for the ceopavn data and one for all of the other data.</p>
<p>If the <b>NcepArchive.pm</b> module is going to be used, a new instance needs to be created with the month and year passed to it.</p>

<ol>
	<li><p class="command">Create the Ceopavn Tar Ball</p>
		<p>Create the tar ball using the following command as a pattern.</p>
		<div class="code">tar -cvf ceopavn.YYYYMM.tar ceopavn.*</div>
		<p>The <b>YYYY</b> is the year of the data and the <b>MM</b> is the month.  A path for the file name may also be specified since the command should be run in the directory where the files are located.  (This prevents directories from being added into the tar ball.</p>
		<p>The module uses the <b>archive_ceopavn()</b> function to perform this task.</p>
		</li>
	<li><p class="command">Remove the Ceopavn Files</p>
		<p>Remove all of the files that were placed into the tar ball.  They are no longer needed since they are not put into the database.  JOSS only archives them on the mass store as a backup.</p>
		<p>The module performs this task at the same time as creating the tar ball.</p>
		</li>
	<li><p class="command">Put the Ceopavn Tar Ball on the Mass Store</p>
		<p>Copy the tar ball onto the mass store using the following command:</p>
		<div class="code">/opt/dcs/bin/msrcp -pe 32767 -wpwd PWD ceopavn.YYYYMM.tar mss:/JOSS/DATA/RAW/BY_PROJECT/CEOP/YYYY/ceopavn.YYYYMM.tar</div>
		<p>The <b>YYYY</b> is the year and <b>MM</b> is the month for the file.  The <b>PWD</b> needs to be replaced with the JOSS mass store password.</p>
		<p>The module uses the <b>mass_store_ceopavn()</b> command to copy the file to the mass store.</p>
		</li>
	<li><p class="command">Create the Katz Tar Ball</p>
		<p>Create the tar ball using the following command pattern.</p>
		
    <div class="code">tar -cvf katz_mmmYYYY.tar mmmYYYY/ > mmmYYYY.log</div>
    <p>This should be run from the directory above the directory where the files 
      are located. The <b>mmmYYYY</b> directory should be included in this tar 
      ball. The <b>mmm</b> is the first three letters of the month (all lowercase) 
      and the <b>YYYY</b> is the year. The log file is created for the e-mail 
      that is explained in two more steps.</p>
		<p>The module does this using the <b>archive_ncep()</b> function.</p>
		</li>
	<li><p class="command">Put the Katz Tar Ball on the Mass Store</p>
		<p>Copy the file to the mass store using the following command:</p>
		<div class="code">/opt/dcs/bin/msrcp -pe 32767 katz_mmmYYYY.tar mss:/DSS/DS507.5/updates/katz_mmmYYYY.tar</div>
		<p>The module uses the <b>mass_store_ncep()</b> function to copy the file.</p>
		</li>
	<li>
    <p class="command">E-mail Chi-Fan the Log File</p>
		
    <p>E-mail Chi-Fan the log file created during the tar ball creation. The information 
      about what to send and his e-mail address can be found in the code of the 
      <b>NcepArchive.pm</b> module. (It will not be placed here to prevent the 
      address from being found by search engines and spammers.)</p>
		<p>The module uses the <b>email_chifan()</b> function to send the form message.</p>
		</li>
</ol>

<a name="process_uncompress"></a><h3>Uncompressing the Data</h3>
<p>Uncompressing the data is the process of either untarring, uncompressing, or both the files that were downloaded so they can be inserted into the database.</p>
<ol>
	<li><p class="command">Untar and Uncompress the Hourly Precipitation Files</p>
		<p>Untar the hourly precip file that was downloaded using the following command:</p>
		<div class="code">tar -xvf prcp.hrly.mmmYYYY</div>
		<p>This will generate a day file for each day of the month.  These need to be uncompressed using the following command.</p>
		<div class="code">uncompress gage.hrly.prcp.*</div>
		<p>The tar ball that was downloaded can be removed if all of the files were extracted and uncompressed cleanly.</p>
		<p>The module does all the entire step with the <b>uncompress_hrly_prcp()</b> function.</p>
		</li>
	<li><p class="command">Untar and Uncompress the Daily Precipitation Files</p>
		<p>This is the same as the hourly, except that the <b>hrly</b> is replaced with <b>dly</b>.</p>
		</li>
	<li><p class="command">Untar the Snapshot Files</p>
		<p>Untar the snapshot file that was downloaded using the following command.</p>
		
    <div class="code">tar -xvf snapshot.mmmYYYY</div>
		<p>This will generate a tar file for each day of the month that will also need to be untarred using the following command.</p>
		
    <div class="code">tar -xvf ST2_vu.*</div>
		<p>This will extract a lot of GIF images.  The snapshot file and the individual day files can be removed if the untarring worked correctly.</p>
		<p>The module uses the <b>uncompress_snapshot()</b> function to extract and remove the files.</p>
		</li>
	<li><p class="command">Untar the Stage IV Files</p>
		<p>Untar the stage IV file that was downloaded using the following command.</p>
		
    <div class="code">tar -xvf stage4.mmmYYYY</div>
		<p>This will generate a tar file for each day of the month that will also need to be untarred using the following command.</p>
		
    <div class="code">tar -xvf ST4*</div>
		<p>The will extract compressed data files and GIF images.  The stage IV file and the day tar balls can be removed if the untarring worked correctly.</p>
		<p>The module uses the <b>uncompress_stage4()</b> function to extract and remove the files.</p>
		</li>
	<li><p class="command">Untar the Stage II 4 KM Files</p>
		
    <p>Untar the ST2_4km files that were downloaded using the following command.</p>
		
    <div class="code">tar -xvf ST2_4km*</div>
		<p>This will generated a number of different type of GRIB files.  The tar balls can removed if the untarring worked correctly.</p>
		<p>The module use the <b>uncompress_st2_4km()</b> function to extract and remove the files.</p>
		</li>
</ol>
<p>The module extracts each type of file into its own directory in the <b>ingest</b> directory where the files were downloaded.  The database scripts assume that the files exist and these directories.  So if the database scripts are going to be used, the files must be placed in those directories.  The following is how the directories should be defined for the database scripts to work properly:</p>
<table>
<tr><th>/ingest/YYYY/MM/dly_prcp</th><td>Contains the uncompressed daily precip files</td></tr>
<tr><th>/ingest/YYYY/MM/hrly_prcp</th><td>Contains the uncompressed hourly precip files</td></tr>
<tr><th>/ingest/YYYY/MM/snapshot</th><td>Contains the GIF files from the snapshot tar ball</td></tr>
<tr><th>/ingest/YYYY/MM/st2_4km</th><td>Contains the GRIB files from the ST2_4km files</td></tr>
<tr><th>/ingest/YYYY/MM/stage4</th><td>Contains the GRIB and GIF files from the stage4 file</td></tr>
</table>

<a name="process_database"></a><h3>Putting the Data in the Database</h3>
<p>Putting the data into the database also includes copying the files from the ingest area where they were uncompressed to the archive location for the database.  The process the modules use is to copy the file to the archive location, insert the file into the database while updating the dates in the dataset, and then removing the file from the ingest area.  (The process is the same for both EmpSQL and MySQL.)</p>
<p class="important">There is too much data for this to be done easily without a script.  If something happened that caused the script not to work, talk to someone who is familiar with the process to straighten the database out.  This can involve a lot of SQL directly to the database to see how far along a script has gotten or to remove entries to start things over.  Nothing should be done without confirmation about what should be done.</p>

<hr>

<a name="notes"></a><h2>Other Notes</h2>
<h3>Uncompressing the Data in the Archive Module</h3>
<p>In the most logical, and probably ideal, sense of the processing, the uncompressing 
  should be done during the inserting of the files into the database. The files 
  should be uncompressed, moved to the final archive directory, and then inserted 
  into the database. Unfortunately, this is not the most feasible. The uncompressing 
  is a slow and intensive process and should not be done on <b>hurricane</b> with 
  the rest of the database stuff, but on <b>tornado</b>. To prevent multiple connections 
  between the two machines in the script, it was decided that the uncompressing 
  should happen separately from the rest of the database processing.</p>
<p>The decision to put the functionality into the <b>NcepArchive.pm</b> module was on the idea that the uncompression was a part of the actual archive process and fit in with the name of the module.  It could have been a module of its own, but this was what was decided at the time the modules were created in an attempt to keep things as simple as possible.  (It was thought that an NcepUncompress module may not be obvious exactly what the functionality would be.)</p>

<h3>File Dates</h3>
<p>As of December 2004, the dates for the files of some of the datasets have changed 
  on how they are inserted into the database. The dates were originally put into 
  the database as the begin date and covered the time the file specified. Now 
  the dates are the end dates and the begin date is determined from the time period 
  covers up to and including the date given in the file name. This was decided 
  after looking at the <a href="/data/gcip_eop/docs/katz_stageII_readme.txt">readme</a> 
  file. (This did not effect the hourly and daily gage datasets.)</p>

<h3>Off-line Datasets</h3>
<p>As of August 2004, the off-line tape datasets are no longer being updated.  This is because all of the data is available in on-line datasets (see the <a href="#setup_datasets">dataset list</a>) and no one was ordering the tapes.  To simplify the process and prevent doing work that was not needed, the data is no longer being put on to tapes.  (The decision was made by Steve Williams and Janine Goldstein.)

<hr>
<div class="foot">Last Updated by Joel Clawson on 2004/12/13</div>

</body>
</html>
